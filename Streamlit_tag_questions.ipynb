{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farouk96/Predict_tags_question/blob/main/Streamlit_tag_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvFu5Bb_4Jq2"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-NQHIEYLMaj",
        "outputId": "5b58468f-39f2-4a8a-aaa0-f4563bfecd08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app_tag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app_tag.py\n",
        "import streamlit as st\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# preprocessing functions\n",
        "def clean_text(text):\n",
        "      text = text.lower()\n",
        "      text = re.sub(r\"what's\", \"what is \", text)\n",
        "      text = re.sub(r\"\\'s\", \" \", text)\n",
        "      text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "      text = re.sub(r\"can't\", \"can not \", text)\n",
        "      text = re.sub(r\"n't\", \" not \", text)\n",
        "      text = re.sub(r\"i'm\", \"i am \", text)\n",
        "      text = re.sub(r\"\\'re\", \" are \", text)\n",
        "      text = re.sub(r\"\\'d\", \" would \", text)\n",
        "      text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "      text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "      text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "      text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "      text = re.sub('\\s+', ' ', text)\n",
        "      text= re.sub('nan',' ',text)\n",
        "      text= re.sub('null',' ',text)\n",
        "      text= re.sub('func',' ',text)\n",
        "      text= re.sub(r'[0-9]', ' ', text) # remove numbers\n",
        "      #text= re.sub(r'(?:^| )\\w(?:$| )', ' ', text)\n",
        "      text = text.strip(' ')\n",
        "      return text\n",
        "\n",
        "token=ToktokTokenizer()\n",
        "punct = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "tags_features=['<python',\n",
        " '<javascript',\n",
        " '<java',\n",
        " '<reactjs',\n",
        " '<html',\n",
        " '<r',\n",
        " '<c#',\n",
        " '<android',\n",
        " '<python-3.x',\n",
        " '<pandas',\n",
        " '<node.js',\n",
        " '<sql',\n",
        " '<php',\n",
        " '<css',\n",
        " '<c++',\n",
        " '<flutter',\n",
        " '<arrays',\n",
        " '<c',\n",
        " '<django',\n",
        " '<angular',\n",
        " '<mysql',\n",
        " '<dataframe',\n",
        " '<typescript',\n",
        " '<jquery',\n",
        " '<swift',\n",
        " '<json',\n",
        " '<laravel',\n",
        " '<vue.js',\n",
        " '<ios',\n",
        " '<firebase',\n",
        " '<amazon-web-services',\n",
        " '<react-native',\n",
        " '<dart',\n",
        " '<postgresql',\n",
        " '<kotlin',\n",
        " '<azure',\n",
        " '<excel',\n",
        " '<numpy',\n",
        " '<spring-boot',\n",
        " '<sql-server',\n",
        " '<list',\n",
        " '<mongodb',\n",
        " '<docker',\n",
        " '<tensorflow',\n",
        " '<regex',\n",
        " '<spring',\n",
        " '<api',\n",
        " '<asp.net-core',\n",
        " '<oracle',\n",
        " '<vba',\n",
        " '<linux',\n",
        " '<string',\n",
        " '<swiftui',\n",
        " '<android-studio',\n",
        " '<loops',\n",
        " '<git',\n",
        " '<matplotlib',\n",
        " '<express',\n",
        " '<powershell',\n",
        " '<bash',\n",
        " '<selenium',\n",
        " '<wordpress',\n",
        " '<kubernetes',\n",
        " '<.net',\n",
        " '<ggplot2',\n",
        " '<database',\n",
        " '<algorithm',\n",
        " '<ruby-on-rails',\n",
        " '<function',\n",
        "'<apache-spark',\n",
        " '<keras',\n",
        " '<web-scraping',\n",
        " '<dictionary',\n",
        " '<google-cloud-firestore',\n",
        " '<ruby',\n",
        " '<visual-studio-code',\n",
        " '<machine-learning',\n",
        " '<discord',\n",
        " '<pyspark',\n",
        " '<csv',\n",
        " '<visual-studio',\n",
        " '<ajax',\n",
        " '<for-loop',\n",
        " '<azure-devops',\n",
        " '<xcode',\n",
        " '<google-sheets',\n",
        " '<tkinter',\n",
        " '<macos',\n",
        " '<scala',\n",
        " '<if-statement',\n",
        " '<.net-core',\n",
        " '<react-hooks',\n",
        " '<windows',\n",
        " '<xml',\n",
        " '<elasticsearch',\n",
        " '<dplyr',\n",
        " '<discord.py',\n",
        " '<mongoose',\n",
        " '<bootstrap-4',\n",
        " '<opencv']\n",
        "def strip_list_noempty(mylist):\n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']\n",
        "def clean_punct(text): \n",
        "    words=token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    for w in words:\n",
        "        if w in tags_features:\n",
        "            punctuation_filtered.append(w)\n",
        "        else:\n",
        "            punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))\n",
        "\n",
        "lemma=WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def lemitizeWords(text):\n",
        "    words=token.tokenize(text)\n",
        "    listLemma=[]\n",
        "    for w in words:\n",
        "        x=lemma.lemmatize(w, pos=\"v\")\n",
        "        listLemma.append(x)\n",
        "    return ' '.join(map(str, listLemma))\n",
        "\n",
        "def stopWordsRemove(text):\n",
        "    \n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    words=token.tokenize(text)\n",
        "    \n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))\n",
        "####################################################################\n",
        "st.title('Question_tags_suggestion')\n",
        "title = st.text_input('Le titre')\n",
        "qst= st.text_area('Posez votre question')\n",
        "\n",
        "if title=='':\n",
        "  st.write(\"Svp,ecrivez le titre.\")\n",
        "elif qst=='':\n",
        "  st.write(\"Svp,ecrivez la question.\")\n",
        "else:\n",
        "  #preprocessing a title & text\n",
        "  lst=[title,qst]\n",
        "  for j in lst:\n",
        "    j = str(j)\n",
        "    j = clean_text(j) \n",
        "    j = clean_punct(j) \n",
        "    j = lemitizeWords(j) \n",
        "    j = stopWordsRemove(j) \n",
        "  vectorizer = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=5000)\n",
        "  title=[title]\n",
        "  qst=[qst]\n",
        "  title= vectorizer.fit_transform(title) \n",
        "  qst= vectorizer.fit_transform(qst)\n",
        "  X= hstack([qst,title])\n",
        "\n",
        "  #Download a best model\n",
        "  with open('/content/drive/MyDrive/ P5_HADJ NACER_Farouk/Best tager questions(model)','rb') as f:\n",
        "    model =pickle.load(f)\n",
        "  st.subheader('Tags:')\n",
        "  st.write(model.predict(X))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHl5CLTyON4H",
        "outputId": "b865c91c-cbd8-4e8e-e243-bee864d796ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.97.61:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.332s\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "your url is: https://tidy-elephant-2.loca.lt\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2021-12-11 17:21:05.901 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 354, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app_tag.py\", line 216, in <module>\n",
            "    st.write(model.predict(X))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 457, in predict\n",
            "    indices.extend(np.where(_predict_binary(e, X) > thresh)[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 100, in _predict_binary\n",
            "    score = np.ravel(estimator.decision_function(X))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\", line 407, in decision_function\n",
            "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 580, in _validate_data\n",
            "    self._check_n_features(X, reset=reset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 396, in _check_n_features\n",
            "    f\"X has {n_features} features, but {self.__class__.__name__} \"\n",
            "ValueError: X has 89 features, but SGDClassifier is expecting 10000 features as input.\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2021-12-11 17:21:41.945 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 354, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app_tag.py\", line 216, in <module>\n",
            "    st.write(model.predict(X))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 457, in predict\n",
            "    indices.extend(np.where(_predict_binary(e, X) > thresh)[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 100, in _predict_binary\n",
            "    score = np.ravel(estimator.decision_function(X))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\", line 407, in decision_function\n",
            "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 580, in _validate_data\n",
            "    self._check_n_features(X, reset=reset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 396, in _check_n_features\n",
            "    f\"X has {n_features} features, but {self.__class__.__name__} \"\n",
            "ValueError: X has 90 features, but SGDClassifier is expecting 10000 features as input.\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2021-12-11 17:23:29.054 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 354, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app_tag.py\", line 216, in <module>\n",
            "    st.write(model.predict(X))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 457, in predict\n",
            "    indices.extend(np.where(_predict_binary(e, X) > thresh)[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 100, in _predict_binary\n",
            "    score = np.ravel(estimator.decision_function(X))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\", line 407, in decision_function\n",
            "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 580, in _validate_data\n",
            "    self._check_n_features(X, reset=reset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 396, in _check_n_features\n",
            "    f\"X has {n_features} features, but {self.__class__.__name__} \"\n",
            "ValueError: X has 106 features, but SGDClassifier is expecting 10000 features as input.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app_tag.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Streamlit tag questions.ipynb",
      "provenance": [],
      "mount_file_id": "10F8aqJFV-rTwgFzqCnWwEq6jza_UIzMw",
      "authorship_tag": "ABX9TyNNEwMidHt/3R8GWFj/lxYl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}